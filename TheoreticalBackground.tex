%%%% Time-stamp: <2012-08-20 17:41:39 vk>

%% example text content
%% scrartcl and scrreprt starts with section, subsection, subsubsection, ...
%% scrbook starts with part (optional), chapter, section, ...
%%\chapter{Example Chapter}

\chapter{Theoretical Background}
%\graphicspath{ {./figures/thesis/} }

\section{The Basics of Structure from Motion}
In this section, we review the fundamental principles of the SfM pipeline. The SfM method is the classical solution to the visual localization problem, and the diagram below outlines the basic steps of the SfM pipeline.

 \myfig{thesis/simplified_sfm_pipeline}%% filename
       {scale=0.4}%% width/height
       {Simplified SfM Pipeline}%% caption
       {Basic SfM Pipeline}%% optional (short) caption for list of figures
       {tb2.1}

We shall describe the basic ideas of each stage of the pipeline, as relevant to the localization problem. 

\subsection{Camera Calibration}
Consider the case of projecting a point in 3D to a the image 2D plane, outlined in the figure below. The 2D point $\mathbf{x}$ can be described by the ray $\lambda[u\ v\ 1]^T$ in projective space.

The 3D point $\mathbf{x}$ can be projected onto the image plane by a projection matrix $\mathbf{P}$, thusly:

\[\mathbf{x} = \mathbf{PX} = \mathbf{K[R|t]X}\]

Note the decomposition of the projection matrix $\mathbf{P}$ as $\mathbf{K[R|t})$, wherein \textbf{K} is called the \emph{ intrinsic matrix}. $\mathbf{[R|t]}$ is a concatenation of the rotation and translation of the camera w.r.t some world coordinate system, called the \emph{extrinsic matrix}.

\myfig{thesis/pinhole_camera}%% filename
{scale=0.6}%% width/height
{Pinhole Camera}%% caption
{Pinhole Camera}%% optional (short) caption for list of figures
{tb2.2}

We model \textbf{K} as follows:

\[\mathbf{K} = \begin{bmatrix}
f & s & c_x\\
0 & af & c_y\\
0 & 0 & 1
\end{bmatrix}\]

$f$ is the focal length, and $c_x, c_y$ is the principal point. $s$ is the shear angle between the axes, and a is the aspect ratio. Multiplying a homogenous coordinate in 3-space will clearly lead to a shift by the principal point, and scaling by $f$ upon perspective division. 

The aim of Camera Calibration is to recover the intrinsics and/or extrinsics. If we denote the 3D points as $\mathbf{X}$, and the corresponding 2D points as $\mathbf{x}$, we can formulate the following set of equations:

\[\mathbf{x}=\lambda\begin{bmatrix}
u\\
v\\
1\end{bmatrix}, \mathbf{P} = \begin{bmatrix}
\mathbf{P_1^T}\\
\mathbf{P_2^T}\\
\mathbf{P_3^T}\end{bmatrix} \]

where $u, v$ are the coordinates of the 2D point, and $\mathbf{P_i^T}$ are transposes of the columns of $\mathbf{P}$. 

\[\mathbf{x} = \mathbf{PX}\]

\[\lambda\begin{bmatrix}
u\\
v\\
1\end{bmatrix} = \begin{bmatrix}
\mathbf{P_1^T}\\
\mathbf{P_2^T}\\
\mathbf{P_3^T}\end{bmatrix}\mathbf{X}\]

solve for $\lambda$, and we can write the above as:

\[\mathbf{P_3^TX}u = \mathbf{P_1^TX}\]
\[\mathbf{P_3^TX}v = \mathbf{P_2^TX}\]

Writing this system of equations as a postmultiplication of the $\mathbf{P_i^T}$'s gives us,

\[\mathbf{0} = \underbrace{\begin{bmatrix}
\mathbf{X^T} & 0 & -\mathbf{X^T}u\\
0 & \mathbf{X^T} & -\mathbf{X^T}v\\
\end{bmatrix}}_{\text{\textbf{A}}} \begin{bmatrix}
\mathbf{P_1}\\
\mathbf{P_2}\\
\mathbf{P_3}\end{bmatrix}\]

If we solve the above equation for more than 6 points, we get more than 12 constraints (each equation contributes 2 constraints). The decomposition to intrinsic and extrinsic components can be acquired by an RQ decomposition, as the rotation \textbf{R} is an orthonormal matrix.

Distortions due to actual lens shapes can be modeled as a simple radial distance-based displacement of the 2d points, as follows:

\[\bar{\mathbf{x}} = [u\ v]^T (1 + k_1r^2 + k_2r^2 + ...)\]

For accurate SfM, it is crucial to get a good estimation of the camera projection matrix. 

\subsection{Feature Extraction}
Features are descriptions of special locations of the image that are invariant under transformations of the image, spatial or otherwise. 

Features can either be learned, or handcrafted. We outline the fundamentals of handcrafted features here as we use only those in our pipeline.
\subsubsection{Keypoint Extraction}
We first have to identify interesting points in the image before we can invariantly describe them. The standard keypoint detector is the Harris detector, upon which most other handcrafted detectors are based. We outline below the central ideas of the Harris detector. 

Without loss of generality, assume that a grayscale image I is used. For a small shift of $[\Delta x \ \Delta y]^T$ of some pixel located at $[x \ y]^T$ in a window W of the image I:

\[f(\Delta x, \Delta y) = \sum_{(x_k, y_k) \in W}^{}(I(x_k, y_k) - I(x_k + \Delta x, y_k + \Delta y))^2\]

is the squared error sum obtained by shifting the window by $[\Delta x \ \Delta y]^T$.

We can approximate $I(x + \Delta x, y + \Delta y)$ by a Taylor expansion and keep only the first order terms. This leads us to the following expression for $f$:

\[f(\Delta x, \Delta y) = \sum_{(x, y) \in W}(I_x(x, y)\Delta x + I_y(x, y) \Delta y)^2\]

Expanding the squares, and rewriting as a quadratic matrix multiplication leads us to:

\[f(\Delta x, \Delta y) \approx [\Delta x \ \Delta y]\underbrace{\sum_{(x, y) \in W}\begin{bmatrix}
I_x^2 & I_xI_y\\
I_xI_y & I_y^2\\
\end{bmatrix}}_{\text{M}}[\Delta x \ \Delta y]^T\]

Here, M is called the \emph{structure tensor}. We use the structure tensor to calculate the \emph{Harris Response} as follows:

\[R = det(M)\ -\ k.trace(M)\]

We choose as keypoint only those locations $[x\ y]$ for which R is high. At these locations, both eigenvalues are high. This means  that the error increases uniformly in all directions, and therefore the point is most likely a corner. 

\subsubsection{Descriptor extraction}
There are several handcrafted feature descriptors which describe points of interest robustly, but SIFT (Shift Invariant Feature Transform) is the foundational one and the most widely used. The SIFT detector+descriptor works basically in the following stages:

SIFT Detector:
\begin{itemize}
\item Create scale pyramid of the image using a Gaussian kernel.
\item Create a DoG (Difference of Gaussians) pyramid by taking the difference between successive scales.
\item Detect extremal points by comparing to 8 neighbours in current, above and below scales.  
\item Filter out locations from previous step further with a Harris-like procedure described in the previous section.
\item Assign as orientation of the point the dominant orientation of its gradient.
\end{itemize}

SIFT Descriptor: We know the scale, location and orientation of each keypoint. 
\begin{itemize}
	\item Around each keypoint, consider a 16x16 window. Subdivide this into 16 4x4 blocks.
	\item For each 4x4 block, create an 8-bin orientation histogram. 
	\item Concatenate the 16 8-bin histograms to get a 128-dimensional vector. This is our final SIFT descriptor. 
\end{itemize}

\subsection{Feature Matching}
Given descriptors for interest points for a pair of images (obtained perhaps with procedures like the ones outlined above), we seek to obtain corresponding points. This procedure is often done in a brute force manner for small images (each descriptor is compared to every other descriptor). However, this method is not feasible for images with a large number of features.

The standard solution to the above problem is to utilize tree methods, such as K-d trees, to speed up the search for a matching descriptor candidate. A popular method is the FLANN (Fast Approximate Nearest Neighbour search algorithm).

A pair of descriptors are compared using the L1 distance (for descriptors such as SIFT, SURF, etc) or the Hamming distance (for binary descriptors such as BRIEF).

The output of this stage in the pipeline is the set of corresponding points of the image pair. 

\subsection{Sparse Reconstruction}
\subsubsection{Triangulation}
Given a set of corresponding points in a two-view setting, we can check for the location of the 3D point that projects on to these correspondences by intersecting rays from these correspondences. This idea is the basis of \emph{triangulating} for the location of the 3d point.

\myfig{thesis/triangulation}%% filename
{scale=0.6}%% width/height
{Triangulation in SfM}%% caption
{Triangulation}%% optional (short) caption for list of figures
{tb2.3}

The 3D point $\mathbf{X}$ projects on to the left image as $\mathbf{x_L} = \mathbf{P_LX}$ and onto the right image as $\mathbf{x_R} = \mathbf{P_RX}$, where $\mathbf{P_L}$ and $\mathbf{P_R}$ are the poses of the left and right images respectively. Since the camera poses and the 2D points are known, we can solve for the 3D point $X$ in the above equations. 

\subsubsection{Refinement of reconstruction}
The 3D points so constructed could have inaccuracies due to measurement errors and drift. A procedure called bundle adjustment is proposed to refine the triangulated points. Its objective function is:

\[\min{\mathbf{a_j,b_i}} \sum_{i=1}^{n} \sum_{j=1}^{m} v_{ij}d(\mathbf{Q(a_j, b_i), x_{ij}})^2\]

Camera $i$ is parameterized by $\mathbf{a_i}$, and $\mathbf{b_j}$ is the $j^{th}$ 3D point.$\mathbf{Q(a_j, b_i)}$ is the predicted position of point $i$ on image $j$, and $\mathbf{d}$ is the euclidean distance.

We can see that Bundle Adjustment minimizes overall reprojection error of each 3D point, and is optimized by the Levenberg-Marquardt algorithm.

\section{The Basics of Convolutional Neural Networks (CNNs)}
\subsection{The Fundamentals of Deep Neural Networks}
Neural networks are essentially a composition of alternating linear and nonlinear functions, nested to an arbitrary depth. These nonlinearities are usually fixed, and are called \emph{activation functions} - eg. Logistic Sigmoid, tanh, Rectified Linear Unit (ReLU). 

\begin{center}
	\begin{tabular}{ |c|c| } 
		\hline
		Logistic Sigmoid & $g(x) = \frac{1}{1 + e^{-x}}$ \\\\
		tanh & $g(x) = \frac{e^{2x} - 1}{e^{2x} + 1}$ \\\\
		ReLU & $g(x)= max(0, x)$ \\ 
		\hline
	\end{tabular}
\end{center}

The nesting level is called \emph{layer} in neural network parlance. 

\myfig{thesis/neural_nets}%% filename
{scale=0.6}%% width/height
{Typical neural network architecture}%% caption
{Neural Net Architecture}%% optional (short) caption for list of figures
{tb2.4} % From DeepAi

The output at layer $l+1$ can be expressed as a function of the previous layer's output as follows:

\[a^{(l+1)} = g(\underbrace{W^{(l+1)}a^{(l)} + b^{(l+1)})}_{z^{(l)}}\]

This deeply composed function has enough degrees of freedom to theoretically fit any target function. The fitting procedure is performed by varying the weight matrix of all the layers ($W_{(l)}$). Gradient descent is the algorithm of choice to arrive at the optimal weights.

Gradient descent relies on varying the weights in the direction of the gradient of the loss function w.r.t the weights - this is the direction of steepest descent. However, to apply this procedure, we must first obtain the gradient of the loss function w.r.t the weights. This is obtained by a procedure christened \emph{backpropagation}.

\myfig{thesis/backprop}%% filename
{scale=0.6}%% width/height
{The Backpropagation Algorithm}%% caption
{Backpropagation}%% optional (short) caption for list of figures
{tb2.5} % From http://neuralnetworksanddeeplearning.com/chap2.html

Backpropagation is actually a sequential application of the \emph{chain rule}. 
If our error function is $L$, then backpropagating to find the weights of the first layer $W_0$ would function as follows:

\[\frac{\partial E}{\partial W^{(0)}} =  \underbrace{\underbrace{\frac{\partial E}{\partial y^{(l)}} \frac{\partial y^{(l)}}{\partial z^{(l)}}}_{\delta^{(l)}} \frac{\partial z^{(l)}}{\partial y^{(l-1)}} \frac{\partial y^{(l-1)}}{\partial z^{(l-1)}}}_{\delta^{(l-1)}} \textellipsis \frac{\partial y^{(1)}}{\partial W_{(0)}}\]

The above architecture can technically learn to fit any function 
$f: \mathbb{R}^n \rightarrow \mathbb{R}^m $.

\subsection{Convolutional Neural Networks}
The standard feedforward neural network works only to fit to vector valued functions. Input of any spatial structure would have to first be unrolled into a 1D vector before it being used in this network. However, if we wanted to preserve spatial relationships during the learning procedure (eg. with images), we would need to formulate a new architecture.

Convolutional Neural Networks are the defacto method to address fitting functions to 2D data where spatial relationships are crucial. Thus, images are the most obvious and abundant sources of data for the CNN. CNNs have been applied successfully to several computer vision tasks such as segmentation, classification, object recognition, etc. and near or even superhuman performance has been achieved in these tasks.

We begin by defining the application of the square shaped convolution operator \emph{H} on image \emph{I} of dimensions \emph{M $X$ N} as follows.

\[Conv(y, x) = \sum_{m=0}^{M}\sum_{n=0}^{N}H(m, n)I(y-m, x-n)\]

We can see that the convolution operation is a sum of the image intensities within the window, weighted by the values of the convolution kernel. For accesses outside the image area, padding (by reflection, zeros, etc.) is performed.

CNNs are composed of three different layers: 
\myfig{thesis/typical_cnn}%% filename
{scale=0.3}%% width/height
{The Typical CNN}%% caption
{Typical CNN}%% optional (short) caption for list of figures
{tb2.6} % From wikipedia
\begin{itemize}
	\item The \textit{Convolution Layer}, where the output is a set of images (called feature maps), that are the results of convolution of the input layer by a set of kernels of a certain dimension. \\
	\item The \textit{Pooling/Subsampling Layer}, where a cluster of points around a each point is considered. The average or maximum of this cluster is taken as the output, thereby reducing the dimensionality of the feature maps. \\ 
	\item The \textit{Fully Connected Layer} flattens the 2D feature maps into 1D vector outputs, which can then be used for conventional tasks such as classification or regression. 
\end{itemize}

\subsection{Semantic Segmentation}
%Cite FCN
Semantic Segmentation is the task of predicting a class label for each pixel of a given image. 
The seminal work is the Fully Convolutional Network (FCN), which uses the features of a classification network (experiments were performed on GoogLeNet, VGG and the AlexNet) to predict a pixel-wise semantic mask for the image. 

It is worth briefly looking into the details of GoogLeNet, VGG and AlexNet which are the basic classificaion networks and then proceed on to the usage of their features to predict the semantic mask via the CNN. 

\subsubsection{AlexNet}
AlexNet is of historical significance to CNN architectures, as it was the first network to win the ImageNet classification challenege and was the first to utilize the GPU to speedup its computations. It also introduced the architectural parlance that is now today's terminology of CNNs. 

\myfig{thesis/AlexNet_Architecture}%% filename
{scale=0.3}%% width/height
{The Architecture of AlexNet}%% caption
{AlexNet}%% optional (short) caption for list of figures
{tb2.7} % From paper

AlexNet is composed of 5 convolutional layers, 3 max-pooling layers, 2 normalization layers, 2 fully connected layers, and 1 softmax layer. Usually, the ReLU activation is used as the preferred nonlinearity. 

\subsection{VGG}









 



















 



















 



 





%% vim:foldmethod=expr
%% vim:fde=getline(v\:lnum)=~'^%%%%\ .\\+'?'>1'\:'='
%%% Local Variables: 
%%% mode: latex
%%% mode: auto-fill
%%% mode: flyspell
%%% eval: (ispell-change-dictionary "en_US")
%%% TeX-master: "main"
%%% End: 
