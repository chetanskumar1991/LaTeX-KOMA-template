%%%% Time-stamp: <2012-08-20 17:41:39 vk>

%% example text content
%% scrartcl and scrreprt starts with section, subsection, subsubsection, ...
%% scrbook starts with part (optional), chapter, section, ...
%%\chapter{Example Chapter}

\chapter{Theoretical Background}
%\graphicspath{ {./figures/thesis/} }

\section{The Basics of Structure from Motion}
In this section, we review the fundamental principles of the SfM pipeline. The SfM method is the classical solution to the visual localization problem, and the diagram \ref{tb2.1} outlines the basic steps of the SfM pipeline.

 \myfig{thesis/simplified_sfm_pipeline}%% filename
       {scale=0.4}%% width/height
       {Simplified SfM Pipeline}%% caption
       {Basic SfM Pipeline}%% optional (short) caption for list of figures
       {tb2.1}

We shall describe the basic ideas of each stage of the pipeline, as relevant to the localization problem. 

\subsection{Camera Calibration}
Consider the case of projecting a point in 3D to a the image 2D plane, outlined in the figure \ref{tb2.2}. The 2D point $\mathbf{x}$ can be described by the ray $\lambda[u\ v\ 1]^T$ in projective space.

The 3D point $\mathbf{x}$ can be projected onto the image plane by a projection matrix $\mathbf{P}$, thus:

\[\mathbf{x} = \mathbf{PX} = \mathbf{K[R|t]X}\]

Note the decomposition of the projection matrix $\mathbf{P}$ as $\mathbf{K[R|t})$, wherein \textbf{K} is called the \emph{ intrinsic matrix}. $\mathbf{[R|t]}$ is a concatenation of the rotation and translation of the camera w.r.t some world coordinate system, called the \emph{extrinsic matrix}.

\myfig{thesis/pinhole_camera}%% filename
{scale=0.6}%% width/height
{Pinhole Camera - taken from \cite{schoenberger2018}}%% caption
{Pinhole Camera}%% optional (short) caption for list of figures
{tb2.2}

We model \textbf{K} as follows:

\[\mathbf{K} = \begin{bmatrix}
f & s & c_x\\
0 & af & c_y\\
0 & 0 & 1
\end{bmatrix}\]

$f$ is the focal length, and $c_x, c_y$ is the principal point. $s$ is the shear angle between the axes, and a is the aspect ratio. Multiplying a homogenous coordinate in 3d-space will lead to a shift by the principal point, and scaling by $f$ upon perspective division. 

The aim of Camera Calibration is to recover the intrinsics and/or extrinsics. If we denote the 3D points as $\mathbf{X}$, and the corresponding 2D points as $\mathbf{x}$, we can formulate the following set of equations:

\[\mathbf{x}=\lambda\begin{bmatrix}
u\\
v\\
1\end{bmatrix}, \mathbf{P} = \begin{bmatrix}
\mathbf{P_1^T}\\
\mathbf{P_2^T}\\
\mathbf{P_3^T}\end{bmatrix} \]

where $u, v$ are the coordinates of the 2D point, and $\mathbf{P_i^T}$ are transposes of the columns of $\mathbf{P}$. 

\[\mathbf{x} = \mathbf{PX}\]

\[\lambda\begin{bmatrix}
u\\
v\\
1\end{bmatrix} = \begin{bmatrix}
\mathbf{P_1^T}\\
\mathbf{P_2^T}\\
\mathbf{P_3^T}\end{bmatrix}\mathbf{X}\]

solve for $\lambda$, and we can write the above as:

\[\mathbf{P_3^TX}u = \mathbf{P_1^TX}\]
\[\mathbf{P_3^TX}v = \mathbf{P_2^TX}\]

Writing this system of equations as a postmultiplication of the $\mathbf{P_i^T}$'s gives us,

\[\mathbf{0} = \underbrace{\begin{bmatrix}
\mathbf{X^T} & 0 & -\mathbf{X^T}u\\
0 & \mathbf{X^T} & -\mathbf{X^T}v\\
\end{bmatrix}}_{\text{\textbf{A}}} \begin{bmatrix}
\mathbf{P_1}\\
\mathbf{P_2}\\
\mathbf{P_3}\end{bmatrix}\]

If we solve the above equation for more than 6 points, we get more than 12 constraints (each equation contributes 2 constraints). The decomposition to intrinsic and extrinsic components can be acquired by an RQ decomposition, as the rotation \textbf{R} is an orthonormal matrix.

Distortions due to actual lens shapes can be modeled as a simple radial distance-based displacement of the 2d points, as follows:

\[\bar{\mathbf{x}} = [u\ v]^T (1 + k_1r^2 + k_2r^2 + ...)\]

For accurate SfM, it is crucial to get a good estimation of the camera projection matrix. 

\subsection{Feature Extraction}
Features are descriptions of special locations of the image that are invariant under transformations of the image, spatial or otherwise. 

Features can either be learned, or handcrafted. We outline the fundamentals of handcrafted features, as only these are potentially relevant in our pipeline to construct pointclouds.

\subsubsection{Keypoint Extraction}
We first have to identify interesting points in the image before we can invariantly describe them. One standard keypoint detector is the Harris detector, upon which most other handcrafted detectors are based. We outline below the central ideas of the Harris detector. 

Without loss of generality, assume that a grayscale image I is used. For a small shift of $[\Delta x \ \Delta y]^T$ of some pixel located at $[x \ y]^T$ in a window W of the image I:

\[f(\Delta x, \Delta y) = \sum_{(x_k, y_k) \in W}^{}(I(x_k, y_k) - I(x_k + \Delta x, y_k + \Delta y))^2\]

is the squared error sum obtained by shifting the window by $[\Delta x \ \Delta y]^T$.

We can approximate $I(x + \Delta x, y + \Delta y)$ by a Taylor expansion and keep only the first order terms. This leads us to the following expression for $f$:

\[f(\Delta x, \Delta y) = \sum_{(x, y) \in W}(I_x(x, y)\Delta x + I_y(x, y) \Delta y)^2\]

Expanding the squares, and rewriting as a quadratic matrix multiplication leads us to:

\[f(\Delta x, \Delta y) \approx [\Delta x \ \Delta y]\underbrace{\sum_{(x, y) \in W}\begin{bmatrix}
I_x^2 & I_xI_y\\
I_xI_y & I_y^2\\
\end{bmatrix}}_{\text{M}}[\Delta x \ \Delta y]^T\]

Here, M is called the \emph{structure tensor}. We use the structure tensor to calculate the \emph{Harris Response} as follows:

\[R = det(M)\ -\ k.trace(M)\]

We choose as keypoint only those locations $[x\ y]$ for which R is high. At these locations, both eigenvalues are high. This means  that the error increases uniformly in all directions, and therefore the point is most likely a corner. 

\subsubsection{Descriptor extraction}
There are several handcrafted feature descriptors which describe points of interest robustly, but SIFT (Shift Invariant Feature Transform) is the foundational one and the most widely used. The SIFT detector+descriptor works basically in the following stages:

SIFT Detector:
\begin{itemize}
\item Create scale pyramid of the image using a Gaussian kernel.
\item Create a DoG (Difference of Gaussians) pyramid by taking the difference between successive scales.
\item Detect extremal points by comparing to 8 neighbours in current, above and below scales.  
\item Filter out locations from previous step further with a Harris-like procedure described in the previous section.
\item Assign as orientation of the point the dominant orientation of its gradient.
\end{itemize}

SIFT Descriptor: We know the scale, location and orientation of each keypoint. 
\begin{itemize}
	\item Around each keypoint, consider a 16x16 window. Subdivide this into 16 4x4 blocks.
	\item For each 4x4 block, create an 8-bin orientation histogram. 
	\item Concatenate the 16 8-bin histograms to get a 128-dimensional vector. This is our final SIFT descriptor. 
\end{itemize}

\subsection{Feature Matching}
Given descriptors for interest points for a pair of images (obtained perhaps with procedures like the ones outlined above), we seek to obtain corresponding points. This procedure is often done in a brute force manner for small images (each descriptor is compared to every other descriptor). However, this method is not feasible for images with a large number of features.

The standard solution to the above problem is to utilize tree methods, such as K-d trees, to speed up the search for a matching descriptor candidate. A popular method is the FLANN (Fast Approximate Nearest Neighbour search algorithm).

A pair of descriptors are compared using the L1 distance (for descriptors such as SIFT, SURF, etc) or the Hamming distance (for binary descriptors such as BRIEF).

The output of this stage in the pipeline is the set of corresponding points of the image pair. 

\subsection{Sparse Reconstruction}
\subsubsection{Triangulation}
Given a set of corresponding points in a two-view setting (fig. \ref{tb2.3}), we can check for the location of the 3D point that projects on to these correspondences by intersecting rays from these correspondences. This idea is the basis of \emph{triangulating} for the location of the 3d point.

\myfig{thesis/triangulation}%% filename
{scale=0.6}%% width/height
{Triangulation in SfM, taken from \cite{schoenberger2018}}%% caption
{Triangulation}%% optional (short) caption for list of figures
{tb2.3}

The 3D point $\mathbf{X}$ projects on to the left image as $\mathbf{x_L} = \mathbf{P_LX}$ and onto the right image as $\mathbf{x_R} = \mathbf{P_RX}$, where $\mathbf{P_L}$ and $\mathbf{P_R}$ are the poses of the left and right images respectively. Since the camera poses and the 2D points are known, we can solve for the 3D point $X$ in the above equations. 

\subsubsection{Refinement of reconstruction}
The 3D points so constructed could have inaccuracies due to measurement errors and drift. A procedure called bundle adjustment is proposed to refine the triangulated points. Its objective function is:

\[\min{\mathbf{a_j,b_i}} \sum_{i=1}^{n} \sum_{j=1}^{m} v_{ij}d(\mathbf{Q(a_j, b_i), x_{ij}})^2\]

Camera $i$ is parameterized by $\mathbf{a_i}$, and $\mathbf{b_j}$ is the $j^{th}$ 3D point.$\mathbf{Q(a_j, b_i)}$ is the predicted position of point $i$ on image $j$, and $\mathbf{d}$ is the euclidean distance.

We can see that Bundle Adjustment minimizes overall reprojection error of each 3D point, and is optimized by the Levenberg-Marquardt algorithm.

\section{The Basics of Convolutional Neural Networks (CNNs)}
\subsection{The Fundamentals of Deep Neural Networks}
Neural networks are essentially a composition of alternating linear and nonlinear functions, nested to an arbitrary depth. These nonlinearities are usually fixed, and are called \emph{activation functions} - eg. Logistic Sigmoid, tanh, Rectified Linear Unit (ReLU). 

\begin{center}
	\begin{tabular}{ |c|c| } 
		\hline
		Logistic Sigmoid & $g(x) = \frac{1}{1 + e^{-x}}$ \\\\
		tanh & $g(x) = \frac{e^{2x} - 1}{e^{2x} + 1}$ \\\\
		ReLU & $g(x)= max(0, x)$ \\ 
		\hline
	\end{tabular}
\end{center}

The nesting level is called \emph{layer} in neural network parlance (fig. \ref{2.4}). 

\myfig{thesis/neural_nets}%% filename
{scale=0.6}%% width/height
{Typical neural network architecture, taken from the DeepAI website}%% caption
{Neural Net Architecture}%% optional (short) caption for list of figures
{tb2.4} % From DeepAi

The output at layer $l+1$ can be expressed as a function of the previous layer's output as follows:

\[a^{(l+1)} = g(\underbrace{W^{(l+1)}a^{(l)} + b^{(l+1)})}_{z^{(l)}}\]

This deeply composed function has enough degrees of freedom to theoretically fit any target function. The fitting procedure is performed by varying the weight matrix of all the layers ($W_{(l)}$). Gradient descent is the algorithm of choice to arrive at the optimal weights.

Gradient descent relies on varying the weights in the direction of the gradient of the loss function w.r.t the weights - this is the direction of steepest descent. However, to apply this procedure, we must first obtain the gradient of the loss function w.r.t the weights. This is obtained by a procedure christened \emph{backpropagation}.

\myfig{thesis/backprop}%% filename
{scale=0.4}%% width/height
{The Backpropagation Algorithm}%% caption
{Backpropagation}%% optional (short) caption for list of figures
{tb2.5} % From http://neuralnetworksanddeeplearning.com/chap2.html

Backpropagation (fig. \ref{tb2.5}) is actually a sequential application of the \emph{chain rule}. 
If our error function is $L$, then backpropagating to find the weights of the first layer $W_0$ would function as follows:

\[\frac{\partial E}{\partial W^{(0)}} =  \underbrace{\underbrace{\frac{\partial E}{\partial y^{(l)}} \frac{\partial y^{(l)}}{\partial z^{(l)}}}_{\delta^{(l)}} \frac{\partial z^{(l)}}{\partial y^{(l-1)}} \frac{\partial y^{(l-1)}}{\partial z^{(l-1)}}}_{\delta^{(l-1)}} \dots \frac{\partial y^{(1)}}{\partial W_{(0)}}\]

The above architecture can technically learn to fit any function 
$f: \mathbb{R}^n \rightarrow \mathbb{R}^m $.

\subsection{Convolutional Neural Networks}
The standard feedforward neural network takes as input 1D vectors. Input of any spatial structure would have to first be unrolled into a 1D vector before it being used in this network. However, if we wanted to preserve spatial relationships during the learning procedure (eg. with images), we would need to formulate a new architecture.

Convolutional Neural Networks are the defacto method to address fitting functions to multi-dimensional data where spatial relationships are crucial. CNNs have been applied successfully to several computer vision tasks such as segmentation, classification, object recognition, etc. and high performance has been achieved in these tasks.

We begin by defining the application of the square shaped convolution operator \emph{H} on image \emph{I} of dimensions \emph{M $X$ N} as follows.

\[Conv(y, x) = \sum_{m=0}^{M}\sum_{n=0}^{N}H(m, n)I(y-m, x-n)\]

We can see that the convolution operation is a sum of the image intensities within the window, weighted by the values of the convolution kernel. For accesses outside the image area, padding (by reflection, zeros, etc.) is performed.

CNNs are composed of three different layers: 
\myfig{thesis/typical_cnn}%% filename
{scale=0.3}%% width/height
{The Typical CNN, taken from Wikipedia}%% caption
{Typical CNN}%% optional (short) caption for list of figures
{tb2.6} % From wikipedia

\begin{itemize}
	\item The \textit{Convolution Layer}, where the output is a set of images (called feature maps), that are the results of convolution of the input layer by a set of kernels of a certain dimension. \\
	\item The \textit{Pooling/Subsampling Layer}, where a cluster of points around a each point is considered. The average or maximum of this cluster is taken as the output, thereby reducing the dimensionality of the feature maps. \\ 
	\item The \textit{Fully Connected Layer} flattens the 2D feature maps into 1D vector outputs, which can then be used for conventional tasks such as classification or regression. 
\end{itemize}

\subsection{Semantic Segmentation}
%Cite FCN
Semantic Segmentation is the task of predicting a class label for each pixel of a given image. The seminal work is the Fully Convolutional Network (FCN), which uses the features of a classification network. Experiments were performed on GoogLeNet (\cite{Szegedy2015}), VGG (\cite{Simonyan2015}) and the AlexNet (\cite{Krizhevsky2012}) to predict a pixel-wise semantic mask for the image. 

It is worth briefly looking into the details of GoogLeNet, VGG and AlexNet which are the basic classificaion networks and then proceed on to the usage of their features to predict the semantic mask via the CNN. 

\subsubsection{AlexNet}
AlexNet (fig. \ref{2.7}) is of historical significance to CNN architectures, as it was the first network to win the ImageNet classification challenge. It also introduced the architectural parlance that is now today's terminology of CNNs. 

\myfig{thesis/AlexNet_Architecture}%% filename
{scale=0.3}%% width/height
{The Architecture of AlexNet, taken from \cite{Krizhevsky2012}}%% caption
{AlexNet}%% optional (short) caption for list of figures
{tb2.7} % From paper

AlexNet is composed of 5 convolutional layers, 3 max-pooling layers, 2 normalization layers, 2 fully connected layers, and 1 softmax layer. Usually, the ReLU activation is used as the preferred nonlinearity. It addresses the problem of overfitting to data by using the principles of Dropout and data augmentation. 

\subsubsection{VGG}

VGG (fig. \ref{tb2.8}) is an AlexNet derivative, but made with the following improvements.

\myfig{thesis/VGG_Architecture}%% filename
{scale=0.2}%% width/height
{The Architecture of VGG, atken from \cite{Simonyan2015}}%% caption
{VGG}%% optional (short) caption for list of figures
{tb2.8} % From paper

VGG, has several differences that separates it from similar models. 
Unlike AlexNet that uses large receptive fields, VGG uses small receptive fields (3x3 kernel with stride 1). The addition of 3 ReLU units makes the decision function more discriminative. There are also fewer parameters. VGG uses 1x1 convolutions to make the decision function behave in a more non-linear manner without changing the receptive fields.
The smaller convolution filters allows VGG to have a larger number of weight layers, which leads to improved performance. This, however, is a feature shared by GoogLeNet, which is delineated in the next section. 

\subsubsection{GoogLeNet}

The main idea behind the GoogleNet architecture (fig. \ref{tb2.10} was the use of the \emph{Inception} module, which created features of different receptive fields and aggregated them. Note the addition of the 1x1 convolutions from the previous layer before convolution with the set of kernels, just to reduce dimensionality of the incoming feature volume. 

\myfig{thesis/Inception_module}%% filename
{scale=0.4}%% width/height
{Inception Module, taken from \cite{Szegedy2015}}%% caption
{Inception Module}%% optional (short) caption for list of figures
{tb2.9} % From OpenCV Tutorials.

The GoogleNet has a depth of 22 layers, with 27 pooling layers. It consists of 9 inception modules stacked linearly in total, where the ends of the inception module is connected to a global average pooling layer. The intuition is that we learn features of varying receptive fields, each of which "zoom-out" with a stacking of an inception module. The below figure depicts pictorially the architecture of the GoogLeNet:

\myfig{thesis/googlenet}%% filename
{scale=0.2}%% width/height
{GoogLeNet Architecture, taken from \cite{Karri2017}}%% caption
{GoogLeNet Architecture}%% optional (short) caption for list of figures
{tb2.10} %

\subsubsection{Fully Convolutional Networks for Semantic Segmentation}
Fully Convolutional Neural Networks (FCNs) by \cite{long2015} in the use of CNNs for Semantic Segmentation.  

FCNs start with a backbone classification network (such as AlexNet, VGG or GoogLeNet which we have described above), but with several adaptations which we will describe now . 

In classification, conventionally, an input image is downsized and goes through the convolution layers and fully connected (FC) layers, and output one predicted label for the input image, as follows:

Conventional classification networks downsize the input image before sending them through the convolution and fully connected (FC) layers before predicting one label for the image, as shown in the below fig. \ref{tb2.11}:

\myfig{thesis/fcn_normal_classification}%% filename
{scale=0.3}%% width/height
{Conventional classification networks, taken from \cite{long2015}}%% caption
{Conventional classification}%% optional (short) caption for list of figures
{tb2.11} %

If we turn the FC layers into 1x1 convolutional layers and the image is not downsized, the output will not be a single label. The output is blown back to the input image resolution by upsampling (fig. \ref{tb2.12}). 

\myfig{thesis/fcn_actual_pred}%% filename
{scale=0.3}%% width/height
{Fully Convolutional Layer, taken from \cite{long2015}}%% caption
{Fully Convolutional Layer}%% optional (short) caption for list of figures
{tb2.12}

Note that we can lose a lot of fine features while going through the pooling operations of the network, so the earlier features containing finer information are added onto the later stages as shown below in fig. \ref{tb2.13}:

\myfig{thesis/fcn_architecture}%% filename
{scale=0.2}%% width/height
{FCN structure, taken from \cite{long2015}}%% caption
{FCN structure}%% optional (short) caption for list of figures
{tb2.13}

Depending on the stage at which we upsample and fuse features, we get FCN-32, FCN-16 and FCN-8's. Obviously, FCN-8's produce the best performance as it gets closest to the input resolution and incorporates fine and coarse feature fusions at several scales. 

FCNs were built on by several authors incorporating more refinement stages (\cite{badrinarayanan2016}), efficient non-linear upsampling  schemes  (\cite{liu2015}),  adding  global  context () \cite{zhao2017})  and pyramid pooling for context aggregation. \cite{yu2016} proposed  a  context  module  the  uses  dilated  convolutions  to enlarge  the  receptive  field.  \cite{chen2017}  proposed  using multiple parallel dilated convolutions with different sampling rates  for  multi-scale  learning  in  addition  to  using  CRFs  for post-processing. \cite{Valada2017} introduced multi-scale residual blocks  with  dilated  convolutions  as  parallel  convolutions  to enable faster inference without compromising the performance.

























 



















 



















 



 





%% vim:foldmethod=expr
%% vim:fde=getline(v\:lnum)=~'^%%%%\ .\\+'?'>1'\:'='
%%% Local Variables: 
%%% mode: latex
%%% mode: auto-fill
%%% mode: flyspell
%%% eval: (ispell-change-dictionary "en_US")
%%% TeX-master: "main"
%%% End: 
