%%%% Time-stamp: <2012-08-20 17:41:39 vk>

%% example text content
%% scrartcl and scrreprt starts with section, subsection, subsubsection, ...
%% scrbook starts with part (optional), chapter, section, ...
%%\chapter{Example Chapter}
\chapter{Methodology}
\label{Methodology}
Our approach to the Visual Localization problem is outlined broadly by these three steps:

\begin{itemize}
	\item Upon a dataset of overlapping map tiles of an area, train a Pose Regression Network (MapNet, \cite{Brahmbhatt2018}) to predict a 2d translation from a reference map tile. \\
	
	\item Convert pointcloud of the urban scene into a representation with building contours as seen from the road. This is the \emph{query} map tile.\\
	
	\item With the trained MapNet, Infer the location of the \emph{query map tile} directly upon the set of map tiles on which MapNet was trained.\\  
\end{itemize}

\myfig{thesis/our_pipeline}%% filename
{scale=0.5}%% width/height
{Our Pipeline}%% caption
{Our Pipeline}%% optional (short) caption for list of figures
{Me1.0} 

The approach can be characterized as a hybrid Pose Regression, wherein the query image is a map-tile like representation. We will expound upon each of the three stages, starting with the details of training our Pose Regression network.   

\section{Pose Regression}
\subsection{MapNet}
MapNet (\cite{Brahmbhatt2018}), is a pose regresssion network that is based on the same idea as PoseNet(\cite{Kendall2015}) which was dealt with in the previous section. However, it mitigates the problem of PoseNet in that it reduces the noisy estimations by incorporating relative motion between frames as geometric constraints in the final loss function. 

\myfig{thesis/mapnet}%% filename
{scale=0.6}%% width/height
{MapNet Structure}%% caption
{MapNet}%% optional (short) caption for list of figures
{Me1.1}

As illustrated in the above figure, MapNet also has additional modules called MapNet+ which incorporates relative pose information from Visual Odometry in the loss function as a further constraint while training. Pose Graph Optimization (PGO) is an additional component that allows MapNet to fuse incoming odometry information with the predictions from the network - this is done by enforcing the condition that pairwise relative poses from odometry must be positioned similar to the poses predicted by the network.

However, we will be using only the basic version of MapNet - that is, the version 
that enforces relative pose constraints between pairs of frame poses (as we get no visual odometry for map tiles).

The \textit{architecture} of MapNet is basically a ResNet-34 modified as follows:
\begin{itemize}
	\item Add global pooling layer after the last convolution layer.
	\item Add a fully convolutional layer of 2048 neurons with ReLU and dropout of 0.5.
	\item Finally, finish with a fully convolutional layer that regresses the 6-dof pose. 
\end{itemize} 

However, the 6-DoF pose that is regressed is a 6 parameter vector. This is due to the fact that the rotation is parametrized as the logarithm of the unit quaternion which results in a better performance than the standard 4 parameter quaternion. 

The loss function is defined as follows:

\[ \mathcal{L}_D(\Theta)= \Sigma_{i=1}^{|\mathcal{D}|}h(p_i,p_i^{*})) + \Sigma_{i,j=1, i\neq j}^{|\mathcal{D}|}h(v_{ij},v_{ij}^{*}) \] 

Where $v_{ij}$ is the relative pose between frames $i$ and $j$. The loss between poses $p_i$, $h(.)$, is defined as

\[ h(p_i,p_i^{*}) = \|t - t^*\|e^{-\beta} + \beta + \|w - w^*\|e^{-\gamma} + \gamma \]

The $\beta$ and $\gamma$ terms balance the relative importance between translation ($t$) and rotation ($w$) loss terms respectively. 

\subsection{The Dataset}

\subsubsection{About the Dataset}
The Dataset that we train MapNet on is drawn from OpenStreetMap (\cite{osm2017}) depending on the city we want to localize in. We constructed two datasets for training the network, taken from sections of the Karlsruhe and Oxford city areas.

\myfig{thesis/osmtrainingmaps}%% filename
{scale=0.35}%% width/height
{Left: Karlsruhe training map, Right: Oxford Training Map. Taken from OpenStreetMap}%% caption
{OSM Training areas}%% optional (short) caption for list of figures
{Me1.2}

The GPS bounds for our training maps are as follows, in latlong pairs. 

Oxford : (51.76868, -1.2730) to (51.73128, -1.192703).\\
Karlsruhe:(49.0443, 8.3362) to (48.9974, 8.4292).

The map is required to be of an urban area is chosen such that the number of buildings in the area is maximized, as building layout is after all the main discriminating feature for the entire pipeline. 

\subsubsection{Obtaining the Map Tiles}
The map, when obtained from OpenStreetMap is downloaded as a .mbtiles database file. One can use any Geographic Information System (GIS) software (we used QGIS), to select a rectangular area on this map containing the area upon which we want to localize and save this area as a set of tiles. In QGIS, this facility can be found under teh \emph{Raster Tools} menu of the Toolbar.

We can choose to save the tiles at whatever zoom-level we want. For the purpose of our experiments, zoom-level 19 is the most suitable as it details approximately a 100 meter section of the road and buildings around it.

\myfig{thesis/osm_zoom_details}%% filename
{scale=0.35}%% width/height
{Open Street Map tiles zoom-level details. Taken from OpenStreetMap}%% caption
{OSM tile zoom details}%% optional (short) caption for list of figures
{Me1.3}

Map tiles obtained thus are saved in a folder, organized as images descending down the y-axis in a subfolder denoting the x-axis going to the right. These xy coordinates have a direct conversion to GPS coordinates, and back as follows:

\begin{minted}{python}
def gps2xy(lat_deg, lon_deg, zoom):
    lat_rad = math.radians(lat_deg)
    n = 2.0 ** zoom
    xtile = float((lon_deg + 180.0) / 360.0 * n)
    ytile = float((1.0 - math.asinh(math.tan(lat_rad)) / math.pi) / 2.0 * n)
    return (xtile, ytile)

def xy2gps(xtile, ytile, zoom):
    n = 2.0 ** zoom
    lon_deg = xtile / n * 360.0 - 180.0
    lat_rad = math.atan(math.sinh(math.pi * (1 - 2 * ytile / n)))
    lat_deg = math.degrees(lat_rad)
    return (lat_deg, lon_deg)
\end{minted}

\subsubsection{Readying the Map Tiles for training}
We cannot use the tiles obtained from OSM directly, as the query tiles we obtain will often contain incomplete information. This is directly due to the fact that the view from the car moving on a road does not contain the entire contour of the buildings around it, only a certain portion of the view. 

\myfig{thesis/carview}%% filename
{scale=0.15}%% width/height
{Incomplete view of buildings from a moving car. Note that only two facades of the building are visible. Taken from the Oxford Robotcar dataset,  \cite{newman2017}}%% caption
{Car typical view}%% optional (short) caption for list of figures
{Me1.4}

In the above figure we see that only two facades of the building are visible, which means that after processing the information from this scene we get an incomplete map tile. To make our system robust to this possibly incomplete information, we endeavour to make the  \textit{input map tiles resemble a rendering from the car point of view}.

\myfig{thesis/tile_road_pov}%% filename
{scale=0.6}%% width/height
{Left: road points sampled on a typical tile. Right: a rendering of the same tile as seen from the road points,  \cite{newman2017}}%% caption
{Tile Road POV}%% optional (short) caption for list of figures
{Me1.5}

The typical map tile contains roads and buildings in separate colours, and thus we can sample points on the road of the tile. From these points, we can check which parts of the buildings are visible by checking where rays shooting out in all directions from the road points intersect the buildings. With this procedure we get the rendering on the right side of the above figure. 

\subsubsection{Assigning poses to tiles}
Conventionally, two successive frames fed to a pose regression network usually see the same scene but for a small forward motion. There is overlap between two successive frames which enables learning motion cues. 

However, two map tiles ordinarily have no overlap. This we can rectify by creating new tiles positioned in between two successive tiles. The higher the number of new tiles we interpolate between any two given tiles, the better the dataset is for training features for a given trajectory.For our purposes, we interpolate 4 new tiles between any two successive tiles positioned at increasing intervals of $0.25 * tilewidth$. 

The sequence in which we present the tiles from training (\textit{trajectory}), starts from the top-left tile, moves to the right and then resumes again from the left once the row of tiles has run out. This, however, can by any continuous sequence of tiles.

\myfig{thesis/tilepose}%% filename
{scale=0.4}%% width/height
{Choose the top-left tile as origin, and assign pose to every other tile. Taken from Google Maps}%% caption
{Tile Pose}%% optional (short) caption for list of figures
{Me1.6}

We then choose the first tile in the trajectory as the origin, and assign the 2d offset from every tile to the pose of the first tile as the pose of that tile. In the 6-dof scheme our tiles are not rotated, scaled or have any freedom in the z-axis. In other words, we only have reduced the 6-dof pose regression to 2-dof pose regression directly on the map raster. 

\subsection{Creating the Query Tile}
We assume the scenario of a car driving through an urban area, with buildings visible on either side. Operating under this assumption, we adhere to the following procedure to generate the query tile:

\begin{itemize}
	\item Acquire pointcloud of the surrounding scene for a distance of approximately 100 meters of traversal.\\
	\item Semantically label pointcloud into buildings and roads.\\
	\item Project buildings onto the road plane. Sample points from the road plane and render building contours as seen from the road. \\
	\item Rotate this map-tile like representation to align with the true north, and scale it to the resolution of zoom-level 19 OSM tiles.
\end{itemize}

We will expound each of these steps.






















































%% vim:foldmethod=expr
%% vim:fde=getline(v\:lnum)=~'^%%%%\ .\\+'?'>1'\:'='
%%% Local Variables: 
%%% mode: latex
%%% mode: auto-fill
%%% mode: flyspell
%%% eval: (ispell-change-dictionary "en_US")
%%% TeX-master: "main"
%%% End: 
