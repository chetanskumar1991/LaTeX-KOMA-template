%%%% Time-stamp: <2012-08-20 17:41:39 vk>

%% example text content
%% scrartcl and scrreprt starts with section, subsection, subsubsection, ...
%% scrbook starts with part (optional), chapter, section, ...
%%\chapter{Example Chapter}
\chapter{Methodology}
\label{Methodology}
Our approach to the Visual Localization problem is outlined broadly by these three steps. See fig \ref{Me1.0}:

\begin{itemize}
	\item Upon a dataset of overlapping map tiles of an area, train a Pose Regression Network (MapNet, \cite{Brahmbhatt2018}) to predict a 2d translation from a reference map tile. \\
	
	\item Convert pointcloud of the urban scene into a representation with building contours as seen from the road. This is the \emph{query} map tile.\\
	
	\item With the trained MapNet, Infer the location of the \emph{query map tile} directly upon the set of map tiles on which MapNet was trained.\\  
\end{itemize}

\myfig{thesis/our_pipeline}%% filename
{scale=0.6}%% width/height
{Our Pipeline: The input images are segmented, and their labels are transferred onto the pointcloud constructed from the scene. This labelled pointcloud is then converted into a map-tile like representation complete with OSM scale and orientation and is used as a query tile to infer the position of the vehicle directly on the map raster.}%% caption
{Our Pipeline}%% optional (short) caption for list of figures
{Me1.0} 

The approach can be characterized as a hybrid Pose Regression, wherein the query image is a map-tile like representation. We will expound upon each of the three stages, starting with the details of training our Pose Regression network.   

As we have built our system on the Oxford Robotcar dataset, we assume the presence of the following sensors:

\begin{itemize}
	\item 2D LIDAR sensors with a field of view of 270 degrees.
	\item Inertial Motion Unit (IMU) equipped with GPS and Gyroscope. 
	\item Multiple cameras covering the entire field of view.
\end{itemize}


\newpage
\section{Pose Regression}
\subsection{MapNet}
MapNet (\cite{Brahmbhatt2018}), is a pose regresssion network that is based on the same idea as PoseNet(\cite{Kendall2015}) which was dealt with in the previous section. However, it mitigates the problem of PoseNet in that it reduces the noisy estimations by incorporating relative motion between frames as geometric constraints in the final loss function. 

\myfig{thesis/mapnet}%% filename
{scale=0.615}%% width/height
{MapNet Architecture. Mapnet considers both absolute and relative poses for pose regression, enforcing the predictions of the DNN to fit both the absolute pose of the ground truth and the relative positions supplied by Visual Odometry and other modalities such as IMU.}%% caption
{MapNet}%% optional (short) caption for list of figures
{Me1.1}

As illustrated in fig \ref{Me1.1}, MapNet also has additional modules called MapNet+ which incorporates relative pose information from Visual Odometry in the loss function as a further constraint while training. 

Pose Graph Optimization (PGO) is an additional component that allows MapNet to fuse incoming odometry information with the predictions from the network - this is done by enforcing the condition that pairwise relative poses from odometry must be positioned similar to the poses predicted by the network.

However, we will be using only the basic version of MapNet - that is, the version 
that enforces relative pose constraints between pairs of frame poses (as we get no visual odometry for map tiles).

The architecture of MapNet is basically a ResNet-34 modified as follows:
\begin{itemize}
	\item Add global pooling layer after the last convolution layer.
	\item Add a fully convolutional layer of 2048 neurons with ReLU and dropout of 0.5.
	\item Finally, finish with a fully convolutional layer that regresses the 6-dof pose. 
\end{itemize} 

However, the 6-DoF pose that is regressed is a 6 parameter vector. This is due to the fact that the rotation is parametrized as the logarithm of the unit quaternion which results in a better performance than the standard 4 parameter quaternion. 

The loss function is defined as follows:

\[ \mathcal{L}_D(\Theta)= \Sigma_{i=1}^{|\mathcal{D}|}h(p_i,p_i^{*})) + \Sigma_{i,j=1, i\neq j}^{|\mathcal{D}|}h(v_{ij},v_{ij}^{*}) \] 

The loss here penalizes any deviation from the absolute pose, and any deviation from the relative pose of the ground truth odometry. Here $v_{ij}$ is the relative pose between frames $i$ and $j$. The loss between poses $p_i$, $h(.)$, is defined as

\[ h(p_i,p_i^{*}) = \|t - t^*\|e^{-\beta} + \beta + \|w - w^*\|e^{-\gamma} + \gamma \]

The $\beta$ and $\gamma$ terms balance the relative importance between translation ($t$) and rotation ($w$) loss terms respectively. 

\subsection{The Dataset}

\subsubsection{About the Dataset}
The Dataset that we train MapNet on is drawn from OpenStreetMap (\cite{osm2017}) depending on the city we want to localize in. We constructed a dataset for training the network, visualization from a section of the Oxford city area, see fig \ref{Me1.2}.

\myfig{thesis/osmtrainingmaps}%% filename
{scale=0.3}%% width/height
{Training map for Oxford area. visualization from OpenStreetMap}%% caption
{OSM Training area}%% optional (short) caption for list of figures
{Me1.2}

The GPS bounds are: (51.76868, -1.2730) to (51.73128, -1.192703).

The map is required to be of an urban area is chosen such that the number of buildings in the area is maximized, as building layout is after all the main discriminating feature for the entire pipeline. 

\subsubsection{Obtaining the Map Tiles}
The map, when obtained from OpenStreetMap is downloaded as a .mbtiles database file. One can use any Geographic Information System (GIS) software (we used QGIS), to select a rectangular area on this map containing the area upon which we want to localize and save this area as a set of tiles. In QGIS, this facility can be found under teh \emph{Raster Tools} menu of the Toolbar.

We can choose to save the tiles at whatever zoom-level we want. For the purpose of our experiments, zoom-level 19 is the most suitable as it details approximately a 100 meter section of the road and buildings around it. See fig \ref{Me1.3} for details on zoom levels:

\myfig{thesis/osm_zoom_details}%% filename
{scale=0.5}%% width/height
{Open Street Map tiles zoom-level details. visualization from OpenStreetMap}%% caption
{OSM tile zoom details}%% optional (short) caption for list of figures
{Me1.3}

Map tiles obtained thus are saved in a folder, organized as images descending down the y-axis in a subfolder denoting the x-axis going to the right. These xy coordinates have a direct conversion to GPS coordinates, and back as follows:

\begin{minted}{python}
def gps2xy(lat_deg, lon_deg, zoom):
    lat_rad = math.radians(lat_deg)
    n = 2.0 ** zoom
    xtile = float((lon_deg + 180.0) / 360.0 * n)
    ytile = float((1.0 - math.asinh(math.tan(lat_rad)) / math.pi) / 2.0 * n)
    return (xtile, ytile)

def xy2gps(xtile, ytile, zoom):
    n = 2.0 ** zoom
    lon_deg = xtile / n * 360.0 - 180.0
    lat_rad = math.atan(math.sinh(math.pi * (1 - 2 * ytile / n)))
    lat_deg = math.degrees(lat_rad)
    return (lat_deg, lon_deg)
\end{minted}

\subsubsection{Readying the Map Tiles for training}
We cannot use the tiles obtained from OSM directly, as the query tiles we obtain will often contain incomplete information. This is directly due to the fact that the view from the car moving on a road does not contain the entire contour of the buildings around it, only a certain portion of the view (fig \ref{Me1.4}). 

\myfig{thesis/carview}%% filename
{scale=0.15}%% width/height
{Incomplete view of buildings from a moving car. Note that only two facades of the building are visible. visualization from the Oxford Robotcar dataset,  \cite{newman2017}}%% caption
{Car typical view}%% optional (short) caption for list of figures
{Me1.4}

In the above figure we see that only two facades of the building are visible, which means that after processing the information from this scene we get an incomplete map tile. To make our system robust to this possibly incomplete information, we endeavour to make the  \textit{input map tiles resemble a rendering from the car point of view}.

\myfig{thesis/tile_road_pov}%% filename
{scale=0.6}%% width/height
{Left: road points sampled on a typical tile. Right: a rendering of the same tile as seen from the road points,  \cite{newman2017}. On the left tile, buildings are colored orange and the road is blue. Red dots are points from where ray tests are made to create the right POV tile.}%% caption
{Tile Road POV}%% optional (short) caption for list of figures
{Me1.5}

The typical map tile contains roads and buildings in separate colours, and thus we can sample points on the road of the tile. From these points, we can check which parts of the buildings are visible by checking where rays shooting out in all directions from the road points intersect the buildings. With this procedure we get the rendering on the right side of \ref{Me1.5}. 

\subsubsection{Assigning poses to tiles}
MapNet assumes that the training image frames are presented in a sequence with poses associated with each frame. Conventionally, two successive frames fed to a pose regression network usually see the same scene but for a small forward motion. There is overlap between two successive frames which enables learning motion cues. 

However, two map tiles ordinarily have no overlap. This we can rectify by creating new tiles positioned in between two successive tiles. The higher the number of new tiles we interpolate between any two given tiles, the better the dataset is for training features for a given trajectory.For our purposes, we interpolate 4 new tiles between any two successive tiles positioned at increasing intervals of 0.25 of the tile width. 

The sequence in which we present the tiles during training, starts from the top-left tile, moves to the right and then resumes again from the left once the row of tiles has run out. This, however, can by any continuous sequence of tiles.

\myfig{thesis/tilepose}%% filename
{scale=0.4}%% width/height
{Choose the top-left tile as origin, and assign pose to every other tile. visualization from Google Maps}%% caption
{Tile Pose}%% optional (short) caption for list of figures
{Me1.6}

We then choose the first tile in the trajectory as the origin, and assign the 2d offset from every tile to the pose of the first tile as the pose of that tile (fig \ref{Me1.6}). In the 6-dof scheme our tiles are not rotated, scaled or have any freedom in the z-axis. In other words, we only have reduced the 6-dof pose regression to 2-dof pose regression directly on the map raster. 

\subsection{Creating the Query Tile}
We assume the scenario of a car driving through an urban area, with buildings visible on either side. Operating under this assumption, we adhere to the following procedure to generate the query tile:

\begin{itemize}
	\item Acquire pointcloud of the surrounding scene for a distance of approximately 100 meters of traversal.\\
	\item Semantically label pointcloud into buildings and roads.\\
	\item Project buildings onto the road plane. Sample points from the road plane and render building contours as seen from the road. \\
	\item Rotate this map-tile like representation to align with the true north, and scale it to the resolution of zoom-level 19 OSM tiles.
\end{itemize}

We will expound upon each of these steps now.

\subsubsection{Acquisition of Pointcloud}

\myfig{thesis/typical_pointcloud}%% filename
{scale=0.3}%% width/height
{Typical LIDAR point cloud. Constructed from the Oxford Robotcar Dataset}%% caption
{LIDAR pointcloud}%% optional (short) caption for list of figures
{Me1.7}

There exist several ways in which we can acquire the pointcloud from a moving car. An inexpensive way to accomplish this would be to use cameras to capture a sequence of frames of the surrounding scene, and exploit geometric constraints between successive frames to construct a 3D representation. This technique is called Visual Odometry, and its foundations lie in Structure from Motion which we have described in some detail in a previous section. \cite{Mur-Artal2018}, \cite{Schops2019}, and \cite{Das2018} are popular state of the art Visual Odometry methods applicable to our scenario to get pointclouds.

However, most Autonomous Driving setups can safely be assumed to possess a LIDAR sensor. The LIDAR sensor gives us depth measurements within a certain range (usually more than 50 meters) at any given time, and one can combine these depth measurements and the vehicle pose at the timestamp from the Inertial Measurement Unit to yield a pointcloud for a sequence of timestamps. 

For the purpose of this thesis, we will assume the presence of LIDAR and IMU sensor data for the construction of the pointcloud (fig. \ref{Me1.7}). 

\subsubsection{Pointcloud Segmentation}

We only require the points composing buildings and roads from the pointcloud to construct our map-tile representation of the scene. To accomplish this goal, we can use one of two techniques: 

\begin{itemize}
	\item Directly label the pointcloud using Pointcloud segmentation methods based on Deep Learning. Popular methods include \cite{qi2016} and \cite{alnaggar2020}.\\
	
	\item Project the pointcloud to segmented camera images with known poses, and infer labels thus. 
\end{itemize}

We do not use the first approach of segmenting the pointcloud directly, as the network has to be retrained to cope with different kinds of urban scenes. The lack of generalization makes it hard to deploy quickly or reliably. 

We use the second approach, opting to go with the MSeg method (\cite{mseg2020}) to semantically segment images. In the ideal case, if the labels were transferred correctly onto the pointcloud we could move directly to the next step of converting it into the query map tile representation.

The intuition is that if we have constructed a pointcloud for a certain time period, then it can be projected to images taken in that time period (provided that their poses are known). We then decide the label of each pointcloud point by voting on which label it projects to on the set of images. More precisely, we outline the pseudocode for labeling below:

\begin{algorithm}
\caption{Algorithm to label pointcloud by projecting to images}
\begin{algorithmic}
\REQUIRE $S = \{S_1, S_2,..S_N\}$, the set of segmented images for associated with the pointcloud.\\
$T = \{T_1, T_2,...T_N\}$, the set of absolute poses corresponding to each image.\\
$P = \{p_1, p_2, ...p_M\}$, the set of 3d points in the pointcloud. \\
\vspace{5 mm}
\textbf{foreach} $p_k$ in $P$:\\
   
   \hspace{5 mm} label-list: initialize an empty list for voting on a label for the 3d point $p_k$\\
    \hspace{5 mm}\textbf{foreach} $T_j$ in $T$:\\
      \hspace{5 mm}\hspace{5 mm}$l$ = $S_j(T_j p_k)$\\
      \hspace{5 mm}\hspace{5 mm}push label $l$ onto the label-list\\
the label of $p_k$ is the label that occurs most in label-list\\
\end{algorithmic}
\end{algorithm}

\myfig{thesis/typical_pointcloud_seg}%% filename
{scale=0.3}%% width/height
{Segmentation of the pointcloud (Red: Building, Green: Road). Note noisy segmentation of trees as building.}%% caption
{LIDAR pointcloud segmentation}%% optional (short) caption for list of figures
{Me1.8}

The voting policy for labelling keypoints helps to mitigate the noise (fig. \ref{Me1.8}), but inaccuracies of the IMU will inevitably lead to mislabelings. We resort to a series of procedures which we will describe in the following section.

\subsubsection{Conversion of the labelled pointcloud to map tile}
Once we have labelled the pointcloud as building and road, we can fit a plane through the road points. For this we use (\cite{zhou2018})'s plane segmentation method.

Once we have the road plane, we can project the building points onto it (fig. \ref{1.9}). The value of the projected pixel will be the "height" of the building point i.e. its distance from the road plane. 

\myfig{thesis/pcl2flat}%% filename
{scale=0.3}%% width/height
{Left: Labelled pointcloud, Right: Projecting pointcloud on to the road plane}%% caption
{Pointcloud and Projection}%% optional (short) caption for list of figures
{Me1.9}

We then sample points from behind buildings on either sides of the road as in fig. \ref{Me2.0}, and do ray-intersection tests in all directions from these points to trace out building contours. This way, we avoid hitting tree and other random noise which we would encounter if we shot our sampling rays from somewhere on the road. 

\myfig{thesis/flat2ray}%% filename
{scale=0.3}%% width/height
{Left: Projected Pointcloud, Right: Sampling of projected pointcloud from points behind the buildings using omnidirectional line-intersections.}%% caption
{Projection and Sampled Projection}%% optional (short) caption for list of figures
{Me2.0}

We then scan each column of this projected image, and keep only the 5 highest valued points, i.e. the 5 points of maximum height (fig \ref{Me2.1}). This strategy is useful to discard noisy tree points which are often lower than the tops of the buildings. 

\myfig{thesis/ray2ref}%% filename
{scale=0.29}%% width/height
{Left: Sampled projection, Right: Noise reduction of sampled projection by keeping highest pixels per column}%% caption
{Sampled projection and Refined sampled projection}%% optional (short) caption for list of figures
{Me2.1}

This strategy does lead to images where most noisy points mixed with building points are removed. However, several images have only trees on one side and no buildings at all or when trees are higher than buildings. In this case, our simple approach will keep the trees as it is only a way to filter noise assuming presence of buildings. 

\pagebreak
\subsubsection{Rotation to align with True North, and scaling to OSM tile resolution}
The map-like representation that we get from the previous step is at an arbitrary resolution, and is not aligned with the OSM map tile direction. To be used for inference, this projected tile has to be aligned in the true north direction. 

As we assume the presence of a gyroscope, we can get the attitude (orientation of the car with respect to the magnetic north) of the car for the duration of the trajectory for which we have constructed our query tile. To align with the \textit{True North} which is the default orientation of the OSM tile, we must add the magnetic declineation of the car to the attitude. The magnetic declineation is dependent on the latitude, and can be queried from most standard GIS libraries. We use \textit{geomap} for this purpose (https://github.com/amiralis/geo-mapper).

\myfig{thesis/ref2osm}%% filename
{scale=0.3}%% width/height
{Left: Final Sampled Projection rotated to align with true north, Right: Corresponding map tile from OSM. Alignment is good.}%% caption
{Final Sampled Projection and Corresponding OSM Tile}%% optional (short) caption for list of figures
{Me2.2}

We must also make sure that our query-tile is at a resolution similar to OSM map tiles. We do this by calculating the extreme points of the road, and accumulate the distance travelled in meters for the map tile (with car measurements, or IMU). With this, we have an estimate of the pixel per meter resolution for our query tile. We will scale this so that it matches OSM map tile resolution which is about 0.298 meters/pixel for the Oxford Dataset (fig. \ref{Me2.2}).

However, it must be noted that our scaling to OSM resolution is not precise as we cannot calculate the exact pixels travelled in case of a non-linear road. This may lead to noisy estimates during inference. 













































































%% vim:foldmethod=expr
%% vim:fde=getline(v\:lnum)=~'^%%%%\ .\\+'?'>1'\:'='
%%% Local Variables: 
%%% mode: latex
%%% mode: auto-fill
%%% mode: flyspell
%%% eval: (ispell-change-dictionary "en_US")
%%% TeX-master: "main"
%%% End: 
