%%%% Time-stamp: <2012-08-20 17:41:39 vk>

%% example text content
%% scrartcl and scrreprt starts with section, subsection, subsubsection, ...
%% scrbook starts with part (optional), chapter, section, ...
%%\chapter{Example Chapter}

\chapter{Related Work}
\section{Localization against a 3D Pointcloud}
Traditionally, the task of Visual Localization is performed using the components of SfM. Given a query image whose pose we are required to find in some 3D reconstruction, the basic sequence of steps followed are:

\begin{itemize}
	\item \emph{Extract descriptors} for the query image.\\
	\item \textit{Match} 2D query image keypoints to 3D Reconstruction points.\\
	\item \textit{Refine pose} of the query image using the 2D-3D matches of the previous step, and the PnP algorithm.\\
\end{itemize}

Descriptor extraction has been detailed in the previous section. We will describe briefly the proceeding two steps. 

\subsection{2D-3D Matching}
Once we have the features of the query image, we can associate them with corresponding image features in the reconstruction. Each image feature has also an association with some 3D point in the reconstruction, and by transitivity we can assert an association between the query keypoints and a 3D point of the reconstruction.

\myfig{thesis/traditional_localization}%% filename
{scale=0.4}%% width/height
{Feature Based Localization, taken from Torsten Sattler's ECCV'18 slides}%% caption
{Feature Based Localization}%% optional (short) caption for list of figures
{tb3.1} % From Torsten Sattler's ECCV 2018 Visual Localization Slides.

A tree structure like the k-d tree is used to hash the reconstruction's features of the reconstruction for rapid matching of the query image features against large reconstructions.

\subsection{Pose Refinement with RANSAC + Perspective-n-Pose}
Now that we know which 3D points are visible from the query image, we can try to figure out an affine transform of the 3D points that yields a minimum reprojection error when projected on the query image. 

\myfig{thesis/PnP_Algorithm}%% filename
{scale=0.4}%% width/height
{Perspective-n-Pose Algorithm, taken from OpenCV Tutorials}%% caption
{PnP Algorithm}%% optional (short) caption for list of figures
{tb3.2} % From OpenCV Tutorials.

The reprojection error objective to be minimized is:
\[argmin_{R, t}\sum_{i=1}^{N}\|\hat{x_i} - x_i\|^2\]

The projection from 3D point to 2D point i is:
\[\mathbf{x_i} = \mathbf{K[R|t]X_i}\]

The minimization is usually performed with the Levenberg-Marquardt algorithm. 

To obtain a robust estimate of our pose, we sample sets of points and choose that pose which has maximum overall inliers (minimum overall reprojection error). This is the basic idea of the RANSAC procedure.

\section{Localization with Global Image Descriptors}
The basic idea of image based localization is to localize a query image by using strategies to find images (of known pose) in a database that are closest to it, and then compute a pose with respect to these set of "close" images. The method of comparison employed between a pair of images here is the distance between their global image descriptors.While localizing against a huge pointcloud, the global descriptors can be used as a preliminary step in identifying a set of candidate images to form 2D-3D associations. 

This reduces the search space considerably for matching query image features. The paper by \cite{Sattler2012} is typical of these class of techniques.

%cite Fast Image Localization by Sattler. 

\subsection{Visual Vocabulary Trees} %cite Scalable Recognition with a Vocabulary Tree David Niste ́r and Henrik Stewe ́nius

\cite{Nister2006} is the seminal publication that exhibited early the possible use of Vocabulary trees to search visual cues. Intuitively speaking, visual words are a histogram over frequently occuring image content.This content is usually characterized by descriptors such as SIFT, etc.
SIFT is the standard handcrafted descriptor for visual words, as they are highly distinctive. The general procedure to build tese visual words is detailed below:

\myfig{thesis/visual_words}%% filename
{scale=0.25}%% width/height
{Construction of the visual words with hierarchical k-means tree, taken from \cite{Nister2006}}%% caption
{Visual Words}%% optional (short) caption for list of figures
{tb3.3} % From citation

\begin{itemize}
	\item For a large, diverse database of images, aggregate all the SIFT descriptors from each image. This will be our training data.\\
	\item Run an initial k-means clustering on the training data and get the k cluster centers.\\
	\item The training dataset is then partitioned into k sets, where each set contains a cluster center and a set of descriptor vectors closest to it.\\
	\item Apply the previous two steps to each of the k sets, a predetermined number of steps. We now have a hierarchical k-means tree.\\
\end{itemize}

In the online phase, when we need to compute a visual word descriptor for an image, all we need to do is take every (SIFT) descriptor of the image and compute 
which of the k clusters of our vocabulary tree is closest to the word. This "closeness" is computed by propagating the descriptor vector down the tree for each 
level-1 node, yielding a score for each node by computing inner products with each descriptor in the tree.

After doing this scoring for all the descriptors in the image and computing a histogram, we have our visual words description for the image. 

\subsection{VLAD}
The VLAD descriptor, by \cite{Arandlejovic2013}, serves as a global image descriptor. It also uses a vocabulary tree, and is computed as follows:

\begin{itemize}
	\item Compute a visual vocabulary tree as described previously, for some database of images. \\
	\item Extract regions with an affine invariant detector.\\
	\item Describe regions with the 128 dimensional SIFT descriptor. \\
	\item Assign each descriptor to one of the k clusters of the vocabulary tree.\\
	\item Compute and accumulate residuals (difference between cluster center and assigned descriptors) for each of the k cluster centers. Formally, this can
	be defined as:
	\[V(i, j) = \sum_{i=0}^{N}a_k(\mathcal(x)_i)(x_i(j) - c_k(j))\]
	where $x_i(j)$ and $c_k(j)$ are the i-th descriptor and k-th cluster center respectively. $a_k(\mathcal(x)_i)$ denotes the membership of the descriptor in 
	cluster center k.\\
	\item Concatenate the final residuals from the k cluster centers into a $k x 128$ dimensional dimensional descriptor, referred to as unnormalized VLAD.
\end{itemize}

The VLAD descriptor can then be normalized with an L2 function, a signed squared root, or the intra-normalization (wherein the VLAD is normalized within each 
cluster before concatenation, before L2 normalization).

\subsection{NetVLAD}
NetVLAD, by \cite{Arandlejovic2015}, seeks to incorporate the VLAD idea into a CNN, which can potentially learn much better features than the handcrafted SIFT descriptor. However, the 
obstacle to including VLAD as a layer would be the hard assignment of a descriptor to one of the clusters , i.e. $a_k(\mathbf{x})$. This is not differentiable. 
To get around this problem, we change the hard assignment to a softer assignment as follows:

\[a_k(\mathbf{x_i}) = \frac{e^{-\alpha\|\mathbf{x_i - c_k}\|}}{\sum_{k'}e^{-\alpha\|\mathbf{x_i - c_k'}\|}}\]

which assigns descriptor $\mathbf{x_i}$ to to cluster $\mathbf{c_k}$ depending on proximity. Expanding squares and cancelling terms, we can rewrite the assignment
function as:

\[a_k(\mathbf{x_i}) = \frac{e^{\mathbf{w_k^T x_k} + b_k}}{\sum_{k'}e^{\mathbf{w_k'^T x_k'} + b_k'}}\]

where $\mathbf{w_k} = w\alpha\mathbf{c_k}$ and $b_k = -\alpha\|\mathbf{c_k}\|^2$. Implementation-wise, this soft-assignment function can be broken up as a convolution
for the linear part, and then a softmax for the normalization. Overall, the implementation can be summed up pictorially as:

\myfig{thesis/netvlad}%% filename
{scale=0.25}%% width/height
{The NetVLAD network architecture, taken from \cite{Arandlejovic2015}}%% caption
{NetVLAD}%% optional (short) caption for list of figures
{tb3.4} % From citation

\section{Localization by Regressing Pose with CNNs}
CNNs trained on image-pose pairs of a scene have enabled the  of poses directly from a query image of that scene. The seminal work on this type of localization is PoseNet.

\subsection{PoseNet}
PoseNet takes as input a 224x224 RGB image, and regresses the 6-DoF pose of that image relative to the scene it was trained on. The output of the PoseNet is a 6 vector $[\mathbf{x}, \mathbf{q}]$, where \textbf{x} is the translation and \textbf{q} is a unit quaternion parameterizing rotation. 

\subsubsection{PoseNet Loss}
The loss function for the CNN of PoseNet is defined as follows:
\[loss(I) = \mathbf{\|\hat{x} - x\|}_2 + \beta\mathbf{\|q - \dfrac{q}{\|q\|}\|}_2\]

Note that the $\beta$ parameter balances the relative importance of translation and rotation loss.

\subsubsection{PoseNet Architecture}
PoseNet, by \cite{Kendall2015} utilizes as backbone the GoogleNet Architecture. GoogleNet is a 22 layer deep CNN which was state of the art at the time for classification.

However, the following modifications were made by the authors of PoseNet to convert the GoogLeNet backbone into a pose regressor network. 

\begin{itemize}
	\item The 3 softmax classifiers were replaced with an affine regressor (fully connected layers) which output a 7-dimension vector representing pose.\\
	
	\item Another fully connected layer of size 2048 was added before the final layer, so it could be used as a feature vector for localization. \\
	
	\item Normalize quaternion orientation vector to unit length at test time. \\ 
\end{itemize}

PoseNet was trained on image, pose pairs obtained from SfM and was able to get competitive results on standard datasets such as Cambridge and 7Scenes. It spawned several variants for visual localization that used its base architecture as a backbone, and we shall explain how we use one such variant (Mapnet) in our approach to solve the localization problem, as detailed in the next chapter. 

\subsection{VLocNet} %cite Deep Auxiliary Learning for Visual Localization and Odometry, valada et.al
An improvement upon PoseNet was VLocNet by \cite{Valada2018}. In order to estimate the global pose of a pair of query frames accurately with respect to some scene, cues from 
Visual Odometry (relative motion between frames) are used jointly with the global pose of the frames. The idea is that the relative motion estimates from VO would help constrict 
the search space considerably. 

Given a pair of frames $(I_t, I_{t-1})$, the network aims to regress the absolute pose $\mathbf{p_t} = \mathbf{(x_t, q_t)}$ and the relative pose 
$\mathbf{p_{t, t-1}} = \mathbf{(x_{t, t-1}, q_{t, t-1})}$ where $\mathbf{x}$ is the translation 3-vector, and $\mathbf{q}$ is the rotation 4-vector. The semantic 
branch predicts $M_t$, the pixelwise semantic mask of C classes for image $I_t$ 

\myfig{thesis/vlocnet}%% filename
{scale=0.4}%% width/height
{VLocNet architecture, taken from \cite{Valada2018}}%% caption
{VLocNet}%% optional (short) caption for list of figures
{rw3.6} % From citation.

\subsubsection{Architecture}
The backbone network for the VLocNet shown above is a modified ResNet-50 network (\cite{He2015}).

\myfig{thesis/ResNet50}%% filename
{scale=0.4}%% width/height
{ResNet50 architecture, taken from \cite{Mahmood2020}}%% caption
{ResNet50}%% optional (short) caption for list of figures
{rw3.7} % From Automatic Hierarchical Classification of Kelps Using Deep Residual Features.

However, the ResNet50 is modified as follows:

\begin{itemize}
	\item Replace conventional ReLUs with ELU activation functions, which have been found to be more robust to noise and are faster to converge.
	
	\myfig{thesis/ReLUvELU}%% filename
    {scale=0.4}%% width/height
    {The ReLU and the ELU activation functions, taken from https://medium.com/hyunjulie/activation-functions-a-short-summary-8450c1b1d426}%% caption
    {ReLU v ELU}%% optional (short) caption for list of figures
	{rw3.8} % https://medium.com/hyunjulie/activation-functions-a-short-summary-8450c1b1d426

	\item After the fifth layer, add a global average pooling layer. Add three inner product layers (\textit{fc1, fc2, fc3}) after, of dimensions 1024, 3, and 4.
\emph{fc2} and \emph{fc3} regress the translation and rotation respectively. 
\end{itemize}

	Instead of directly regressing the pose from the network, an following extra step is introduced: the fusion of the previous timestep's final downsampling layer output
with that of the current timestep's final downsampling layer output. This happens for pairs of frames in a sequence, and forces the network to learn motion cues in the sequence.

This scheme in combination with the \emph{Geometric consistency loss} described in the following section enables the network to learn motion-specific cues in the 
temporal dimension.

\subsubsection{Learning Pose Regression}
The usual euclidean loss between predicted and ground truth pose is augmented here with an additional term to constrain the error between the relative motion predicted
by the odometry stream of the network, and that available from the ground truth. If we denote our neural network as \textbf{\emph{f}}, and its parameters as $\theta$, we 
can build up to the final loss function as follows:

The translational and rotational losses between two consecutive frames are defined as:
\[\mathcal{L}_{xRel}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t)) = \|\mathbf{x}_{t, t-1} - (\hat{\mathbf{x}}_t - \hat{\mathbf{x}}_{t-1})\|\]
\[\mathcal{L}_{qRel}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t)) = \|\mathbf{q}_{t, t-1} - (\hat{\mathbf{q}}_t - \hat{\mathbf{q}}_{t-1})\|\]
 
These two losses are each weighted exponentially with a learnable exponent, and added together to form the relative loss.
 
\[ \mathcal{L}_{Rel}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t)) = \mathcal{L}_{xRel}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t))exp(-\hat{s}_{x_{Rel}}) + \hat{s}_{x_{Rel}}
+ \mathcal{L}_{qRel}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t))exp(-\hat{s}_{q_{Rel}}) + \hat{s}_{q_{Rel}}\]
 
The absolute loss between pose prediction and ground truth has a similar structure:
\[\mathcal{L}_{x}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t)) = \|\mathbf{x}_{t} - \hat{\mathbf{x}}_t\|\]
\[\mathcal{L}_{q}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t)) = \|\mathbf{q}_{t} - \hat{\mathbf{q}}_t\|\]
 
The individual euclidean translation and rotation losses are put together in a cumulative absolute pose loss terms as follows:
\[ \mathcal{L}_{Euc}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t)) = \mathcal{L}_{x}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t))exp(-\hat{s}_{x}) + \hat{s}_{x}
+ \mathcal{L}_{q}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t))exp(-\hat{s}_{q}) + \hat{s}_{q}\]

We put both relative and absolute losses together to get the following loss term:
\[\mathcal{L}_{Loc}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t)) = \mathcal{L}_{Rel}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t)) + \mathcal{L}_{Euc}(\mathbf{\mathit{f}}(\theta|\mathit{I}_t))\]

\subsubsection{Learning Visual Odometry}
Another branch of the overall network learns to predict visual odometry $\mathbf{p}_{t, t-1} = (\mathbf{x}_{t, t-1}, \mathbf{q}_{t, t-1})$, 
given a pair of frames in a sequence $(I_t, I_{t-1})$. This is done by employing a dual stream architecture, wherein each branch is identical to 
one another, and is based on the ResNet-50 architecture. 
 
The feature maps before the last downsampling stage of both stages are concatenated, convolved through the last residual block, followed by an inner product layer
and two pose regressors for estimating pose of both frames. This loss between predicted and ground truth motion is expressed by the following loss function:
\[ \mathcal{L}_{vo}(\mathbf{\mathit{f}}(\theta|(\mathit{I}_t, \mathit{I}_{t-1})
)) = \mathcal{L}_{x}(\mathbf{\mathit{f}}(\theta|(\mathit{I}_t, \mathit{I}_{t-1})
))exp(-\hat{s}_{x_{vo}}) + \hat{s}_{x_{vo}}
+ \mathcal{L}_{q}(\mathbf{\mathit{f}}(\theta|(\mathit{I}_t, \mathit{I}_{t-1})
))exp(-\hat{s}_{q_{vo}}) + \hat{s}_{q_{vo}}\]
  
The parameters between the odometry network here, and the global pose regression network are shared. This sharing enables an inductive transfer of information between
both networks. 

\subsubsection{Learning Semantics}
The authors propose two variants of a semantic learning branch: a single task architecture that predicts a pixel-wise segmentation mask for a monocular image, 
and a multitask architecture that incorporates self-supervised warping and adaptive fusion layers in the segmentation process. 


For the \emph{single-task architecture}, an encoder-decoder model is used. The encoder is a ResNet-50 architecture, which learns highly discriminative semantic features
16 times downsampled at the output layer. The decoder consists of two deconvolution layers, a skip convolution from the encoder for fusing high-resolution semantic maps
and upsampling to the input feature resolution. At the output of the network, we get classification scores per pixel. We get the probability of assigning a class to 
a pixel given an image, by applying softmax to the per pixel classfication scores. The loss function is therefore defined as the maximum log-sum of all points in this distribution. 


If we have a set of training images, $\mathcal{T} = {(I_n, M_n), n = 1,...N}$, where $I_n = {u_r | r = 1,...\rho}$ is the set of training images consisting of $\rho$ pixels, and $M_n = {m^n_r | r = 1,...\rho}$ is the set of corresponding ground truth masks with a semantic label per pixel, $m_n^r = {1,...C}$.

Using the per-pixel classification scores $s_j$ we can model the probability of a pixel being assigned a semantic class with the softmax function, as follows:

\[p_j(u_r, \theta | I_n) = \frac{exp(s_j(u_r, \theta)}{\sum_{k}^{C}exp(s_k(u_r, \theta)}\]

The optimal network parameters $\theta$ is estimated by minimizing the following loss function:

\[\mathcal{L_seg}(\mathcal{T}, \theta) = \sum_{n=1}^{N}\sum_{r=1}^{\rho}\sum_{j=1}^{C} \delta_{m_n^r, j}p_j(u_r, \theta | I_n)\]

for $(I_n, M_n) \in \mathcal{T}$.

The \emph{multitask architecture} utilizes a self-supervised warping method, wherein the pose of the previous timestep's image as predicted by the odometry stream is utilized.
A depth image of the previous timestep, $D_t$, is predicted using a separate network \cite{Mayer2016}, and the previous image's feature maps (outputs from layers \emph{Res4f} and \emph{Res5c} of the previous timesteps - this is marked in \figurename{rw3.7} ) are now warped onto the current
image's features. 

Formally, if the projection function is $\pi$, we can warp on the previous timestep's pixels $u_r$ onto the current timestep $\hat{u_r}$ with relative pose $p_{1,t-1}$

\[\hat{u_r} = \pi(T(p_{1,t-1}) \pi^{-1}(u_r, D_t(u_r)))\]


This operation allows robustness to camera angle deviations, object scale and frame distortions. It is also a feature augmenter, and thereby enforces consistent
learning of consistent semantics.

\subsubsection{Summary}
The final loss function is an accumulation of the semantic, odometry and absolute pose losses as defined above. The reasons behind training absolute pose, odometry and semantics 
can be articulated with two points: to enable the absolute pose regression network to encode geometric an semantic information while training, and to enable inductive
transfer between domain specific information. 

This is the first network wherein the results of Localization is consistently equal to or better than the SfM/Feature desriptor approach. For detailed analysis of the results
please refer the paper. 

\section{Cross-View Localization}
Cross-View Localization is the task of localizing a query image against a database of images captured from a different view than the typical query image. The most usual example is localizing a ground-view image against a database of aerial/satellite imagery.

%https://openaccess.thecvf.com/content_cvpr_workshops_2015/W13/papers/Workman_On_the_Location_2015_CVPR_paper.pdf
The seminal work in this area submitted by \cite{Jacobs2015}, where they fine-tuned AlexNet on ImageNet and Places datasets. They proved the efficacy of CNNs as a feature extractor for cross-view image association by creating a dataset of the Charleston San Francisco aresa as follows: aerial imagery for a predetermined 40 sq.km region was downloaded from Bing maps, and was associated by GPS tagging to ground level images obtained via various open-source image databases such as flickr and Google StreetView. The pre-trained AlexNet was fed these images, and the final layer output is taken as the corresponding feature vector. 

%https://www.cc.gatech.edu/~hays/papers/geolocalization_cvpr13_0421.pdf
Their experiments were able to prove the high discriminative nature of CNNs for this purpose. The following graphs from their paper shows the performance of CNN features against the state of the art method by Lin.et.al (\cite{Lin2013}) at the time (which sought to learn the relationship between cross-view image pairs with a Support Vector machine based method).

\myfig{thesis/jacobs_CNN}%% filename
{scale=0.25}%% width/height
{AlexNet features (Jacobs et.al) vs SVM learned (Lin et.al) Cross-View Localization, taken from \cite{Jacobs2015}}%% caption
{CNN for cross view Jacobs et.al.}%% optional (short) caption for list of figures
{rw3.9} % https://medium.com/hyunjulie/activation-functions-a-short-summary-8450c1b1d426

Other methods that utilize CNNs for Cross View localization were then spawned. These myriad methods each utilize a different backbone CNN architecture, and incorporated more information about the image to make the CNN output more distinctive features.

Workman et al. submitted that fine- tuning the aerial branch by minimizing the distance between aerial and ground images results in improved localization performance.  Vo and Hays (Vo and Hays 2016) conducted thorough evaluations on the network suited best for cross-view localization: i.e. binary classification (image retrieval), Triplet or Siamese CNNs. Hu et.al stacked a NetVLAD layer upon a VGG Network to endow VGG features the viewpoint invariance that NetVLAD possesses.Liu and Li (2019) added per-pixel orientation information into their CNN so that the features learned are sensitive to pixel orientation as another discriminative feature. 

We outline here a method that is state of the art in the Cross View Localization domain, and builds upon the idea of using a CNN to learn features. 

\subsection{Optimal Feature Transport for Cross-View Image Geo-Localization}
This paper by \cite{Shi2019}. deals with cross-view geo localization, the task of finding the location of a given ground-view image in a large satellite map. As we have detailed before, this paper too is based on the premise of using DCNNs to extract features that can be used for comparing a ground-view image against an aerial image database. There are however two insights that are incorporated by the authors:

\begin{itemize}
	\item Spatial layout of the features is to be taken into account - i.e. their relative position with respect to other features in the image plane. 
	
	\item Take into consideration that the ground and aerial images are, for the lack of a better word, of different "domains". 
\end{itemize}

To tackle both of these problems, the direct approach of learning a transport matrix to transfer the ground-domain features to the aerial domain features is utilized. 

\myfig{thesis/cvft}%% filename
{scale=0.3}%% width/height
{The Cross View Feature Transport Module, taken from \cite{Shi2019}}%% caption
{CVFT}%% optional (short) caption for list of figures
{rw4.0} % cite paper

Two CNN branches (VGG, \cite{Simonyan2015}.) are used to learn the aerial and ground features, but the improvisation is in the inclusion of the new feature transport module. Previous approaches are similar upto the usage of two CNN branches to extract features for the aerial and ground images. However they use a feature aggregation such as pooling is used before minimizing the loss between features, which discards spatial layout information.

\myfig{thesis/cvft_old}%% filename
{scale=0.3}%% width/height
{Older Cross-View localization architectures, taken from \cite{Shi2019}}%% caption
{Previous Cross-View Architecture}%% optional (short) caption for list of figures
{rw4.1} % cite paper

The feature transport module is actually a set of cost matrices that transform ground to aerial features. This does away with the lossy aggregation operation, and explicitly models the domain change.

\subsubsection{Feature Transport}
$\mathbf{f}^i(g) \in R^{h \times w}$ and $\mathbf{f}^i(a) \in R^{h \times w}$ indicate the ground and aerial feature maps, wherein $i$ is the feature channel number an $h$ and $w$ indicate the width and height of the features.

Ideally, both the ground and aerial images must be used in the computation of the cost matrix $\mathbf{C}$. For purposes of efficiency, we postpone the inclusion of aerial images to the loss function and instead use a regression block to compute $\mathbf{C}$.

Our transport matrix $\mathbf{P}$, has to satisfy the following strictly convex loss condition:

$\mathbf{P}^* = argmin_{P} \ <\mathbf{P}, \mathbf{C}>_F - \lambda h(\mathbf{P})
$

where $h$ is the entropy regularization, and the norm is the Frobenius norm. To minimize this particular functional objective, the Sinkhorn algorithm is utilized. The Sinkhorn algorithm first applies an exponential kernel on $\mathbf{C}$ and then iteratively converts $\mathbf{C}$ to a doubly stochastic matrix (rows and columns add up to one). If row and column normalizations are denoted as:

\[\mathbf{C}' = exp(-\lambda C)\]
\[\mathcal{N}^r_{i, j} = \frac{c'_{i,j}}{\sum_{k=1}^{N} c'_{k,j}} \]
\[\mathcal{N}^c_{i, j} = \frac{c'_{i,j}}{\sum_{k=1}^{N}c'_{i,k}} \]

For the m-th iteration, the Sinkhorn operation is given as 

\[\mathbf{S}_m(\mathbf{C}') = \mathcal{N}^r(\mathcal{N}^c \mathcal{S}_{m-1}(\mathbf{C}'))\]

So when the iterations converge, $\mathbf{P}^* = \mathbf{S}_m(\mathbf{C}')$.

It is to be noted that the Sinkhorn operation is differentiable. Thus, when the ground feature maps are transported using the $\mathbf{P}^*$ and the loss is backpropagated, the domain transfer is also learned.

\[\mathcal{L}_{triplet} = log(1 + e^{\gamma (d_{pos} - d_{neg}})\]

where $d_{pos}$ and $d_{neg}$ denote the l2 distance of all the positive and negative aerial features from the anchor ground feature. The network outperforms the state of the art at the time of its writing (CVM-Net, \cite{Hu2018}), as outlined in the graph below:

%Cite CVM Net

\myfig{thesis/cvft_experiment}%% filename
{scale=0.3}%% width/height
{Performance of CVFT against CVM-Net on the CVUSA and CVACT datasets respectively, taken from \cite{Shi2019}}%% caption
{CVFT Experiments}%% optional (short) caption for list of figures
{rw4.2} % cite paper







%% vim:foldmethod=expr
%% vim:fde=getline(v\:lnum)=~'^%%%%\ .\\+'?'>1'\:'='
%%% Local Variables: 
%%% mode: latex
%%% mode: auto-fill
%%% mode: flyspell
%%% eval: (ispell-change-dictionary "en_US")
%%% TeX-master: "main"
%%% End: 
