%%%% Time-stamp: <2012-08-20 17:41:39 vk>

%% example text content
%% scrartcl and scrreprt starts with section, subsection, subsubsection, ...
%% scrbook starts with part (optional), chapter, section, ...
%%\chapter{Example Chapter}

\chapter{Theoretical Background}
%\graphicspath{ {./figures/thesis/} }

\section{The Basics of Structure from Motion}
In this section, we review the fundamental principles of the SfM pipeline. The SfM method is the classical solution to the visual localization problem, and the diagram below outlines the basic steps of the SfM pipeline.

 \myfig{thesis/simplified_sfm_pipeline}%% filename
       {scale=0.4}%% width/height
       {Simplified SfM Pipeline}%% caption
       {Basic SfM Pipeline}%% optional (short) caption for list of figures
       {tb2.1}

We shall describe the basic ideas of each stage of the pipeline, as relevant to the localization problem. 

\subsection{Camera Calibration}
Consider the case of projecting a point in 3D to a the image 2D plane, outlined in the figure below. The 2D point $\mathbf{x}$ can be described by the ray $\lambda[u\ v\ 1]^T$ in projective space.

The 3D point $\mathbf{x}$ can be projected onto the image plane by a projection matrix $\mathbf{P}$, thusly:

\[\mathbf{x} = \mathbf{PX} = \mathbf{K[R|t]X}\]

Note the decomposition of the projection matrix $\mathbf{P}$ as $\mathbf{K[R|t})$, wherein \textbf{K} is called the \emph{ intrinsic matrix}. $\mathbf{[R|t]}$ is a concatenation of the rotation and translation of the camera w.r.t some world coordinate system, called the \emph{extrinsic matrix}.

\myfig{thesis/pinhole_camera}%% filename
{scale=0.6}%% width/height
{Pinhole Camera}%% caption
{Pinhole Camera}%% optional (short) caption for list of figures
{tb2.2}

We model \textbf{K} as follows:

\[\mathbf{K} = \begin{bmatrix}
f & s & c_x\\
0 & af & c_y\\
0 & 0 & 1
\end{bmatrix}\]

$f$ is the focal length, and $c_x, c_y$ is the principal point. $s$ is the shear angle between the axes, and a is the aspect ratio. Multiplying a homogenous coordinate in 3-space will clearly lead to a shift by the principal point, and scaling by $f$ upon perspective division. 

The aim of Camera Calibration is to recover the intrinsics and/or extrinsics. If we denote the 3D points as $\mathbf{X}$, and the corresponding 2D points as $\mathbf{x}$, we can formulate the following set of equations:

\[\mathbf{x}=\lambda\begin{bmatrix}
u\\
v\\
1\end{bmatrix}, \mathbf{P} = \begin{bmatrix}
\mathbf{P_1^T}\\
\mathbf{P_2^T}\\
\mathbf{P_3^T}\end{bmatrix} \]

where $u, v$ are the coordinates of the 2D point, and $\mathbf{P_i^T}$ are transposes of the columns of $\mathbf{P}$. 

\[\mathbf{x} = \mathbf{PX}\]

\[\lambda\begin{bmatrix}
u\\
v\\
1\end{bmatrix} = \begin{bmatrix}
\mathbf{P_1^T}\\
\mathbf{P_2^T}\\
\mathbf{P_3^T}\end{bmatrix}\mathbf{X}\]

solve for $\lambda$, and we can write the above as:

\[\mathbf{P_3^TX}u = \mathbf{P_1^TX}\]
\[\mathbf{P_3^TX}v = \mathbf{P_2^TX}\]

Writing this system of equations as a postmultiplication of the $\mathbf{P_i^T}$'s gives us,

\[\mathbf{0} = \underbrace{\begin{bmatrix}
\mathbf{X^T} & 0 & -\mathbf{X^T}u\\
0 & \mathbf{X^T} & -\mathbf{X^T}v\\
\end{bmatrix}}_{\text{\textbf{A}}} \begin{bmatrix}
\mathbf{P_1}\\
\mathbf{P_2}\\
\mathbf{P_3}\end{bmatrix}\]

If we solve the above equation for more than 6 points, we get more than 12 constraints (each equation contributes 2 constraints). The decomposition to intrinsic and extrinsic components can be acquired by an RQ decomposition, as the rotation \textbf{R} is an orthonormal matrix.

Distortions due to actual lens shapes can be modeled as a simple radial distance-based displacement of the 2d points, as follows:

\[\bar{\mathbf{x}} = [u\ v]^T (1 + k_1r^2 + k_2r^2 + ...)\]

For accurate SfM, it is crucial to get a good estimation of the camera projection matrix. 

\section{Feature Extraction}
Features are descriptions of special locations of the image that are invariant under transformations of the image, spatial or otherwise. 

Features can either be learned, or handcrafted. We outline the fundamentals of handcrafted features here as we use only those in our pipeline.
\subsection{Keypoint Extraction}
We first have to identify interesting points in the image before we can invariantly describe them. The standard keypoint detector is the Harris detector, upon which most other handcrafted detectors are based. We outline below the central ideas of the Harris detector. 

Without loss of generality, assume that a grayscale image I is used. For a small shift of $[\Delta x \ \Delta y]^T$ of some pixel located at $[x \ y]^T$ in a window W of the image I:

\[f(\Delta x, \Delta y) = \sum_{(x_k, y_k) \in W}^{}(I(x_k, y_k) - I(x_k + \Delta x, y_k + \Delta y))^2\]

is the squared error sum obtained by shifting the window by $[\Delta x \ \Delta y]^T$.

We can approximate $I(x + \Delta x, y + \Delta y)$ by a Taylor expansion and keep only the first order terms. This leads us to the following expression for $f$:

\[f(\Delta x, \Delta y) = \sum_{(x, y) \in W}(I_x(x, y)\Delta x + I_y(x, y) \Delta y)^2\]

Expanding the squares, and rewriting as a quadratic matrix multiplication leads us to:

\[f(\Delta x, \Delta y) \approx [\Delta x \ \Delta y]\underbrace{\sum_{(x, y) \in W}\begin{bmatrix}
I_x^2 & I_xI_y\\
I_xI_y & I_y^2\\
\end{bmatrix}}_{\text{M}}[\Delta x \ \Delta y]^T\]

Here, M is called the \emph{structure tensor}. We use the structure tensor to calculate the \emph{Harris Response} as follows:

\[R = det(M)\ -\ k.trace(M)\]

We choose as keypoint only those locations $[x\ y]$ for which R is high. At these locations, both eigenvalues are high. This means  that the error increases uniformly in all directions, and therefore the point is most likely a corner. 

\subsection{Descriptor extraction}
There are several handcrafted feature descriptors which describe points of interest robustly, but SIFT (Shift Invariant Feature Transform) is the foundational one and reliable even to this day. The SIFT detector+descriptor works basically in the following stages:

SIFT Detector:
\begin{itemize}
\item Create scale pyramid of the image using a Gaussian kernel.
\item Create a DoG (Difference of Gaussians) pyramid by taking the difference between successive scales.
\item Detect extremal points by comparing to 8 neighbours in current, above and below scales.  
\item Filter out locations from previous step further with a Harris-like procedure described in the previous section.
\item Assign as orientation of the point the dominant orientation of its gradient.
\end{itemize}

SIFT Descriptor: We know the scale, location and orientation of each keypoint. 
\begin{itemize}
	\item Around each keypoint, consider a 16x16 window. Subdivide this into 16 4x4 blocks.
	\item For each 4x4 block, create an 8-bin orientation histogram. 
	\item Concatenate the 16 8-bin histogram, to get a 128-dimensional vector. This is our final SIFT descriptor. 
\end{itemize}

 



















 



 





%% vim:foldmethod=expr
%% vim:fde=getline(v\:lnum)=~'^%%%%\ .\\+'?'>1'\:'='
%%% Local Variables: 
%%% mode: latex
%%% mode: auto-fill
%%% mode: flyspell
%%% eval: (ispell-change-dictionary "en_US")
%%% TeX-master: "main"
%%% End: 
